# ğŸ“„ Gemma 4B + Ollama + AnythingLLM Kurulum ve Entegrasyon SÃ¼reci

Bu belge, Ollama Ã¼zerinden indirilen `gemma3:4b` modelinin local olarak nasÄ±l Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nÄ± ve AnythingLLM ile nasÄ±l entegre edildiÄŸini adÄ±m adÄ±m aÃ§Ä±klamaktadÄ±r. AmaÃ§, local bir LLMâ€™in nasÄ±l serve edildiÄŸini ve AnythingLLM'in bu LLM ile nasÄ±l iletiÅŸim kurduÄŸunu teknik dÃ¼zeyde gÃ¶stermektir.

---

## ğŸ”¹ 1. LLM Kurulumu (Ollama ile)

### Ollama'nÄ±n Kurulumu  
Sistemime uygun Ollama kurulum paketi resmÃ® web sitesinden indirildi ve yÃ¼klendi. YÃ¼kleme sonrasÄ± terminalden doÄŸrulama yapÄ±ldÄ±:

**ollama --version** komutu ile test edildi.

**ollama run gemma3:4b** Bu komut LLM local olarak varsa Ã§alÄ±ÅŸtÄ±rÄ±yor aksi takdirde bu LLM'i kuruyor.Bu komutla model http://localhost:11434 Ã¼zerinden HTTP API olarak Ã§alÄ±ÅŸmaya baÅŸladÄ±. Bu endpoint Ã¼zerinden yapÄ±lan her istek modele iletilir ve yanÄ±t dÃ¶nÃ¼lÃ¼r.

- Not: Ollama, modeli local olarak serve eder. DÄ±ÅŸ bir servise ihtiyaÃ§ duymadan kendi kendine Ã§alÄ±ÅŸÄ±r.

## ğŸ”¹ 2. Anything LLM Kurulumu

- AnythingLLM GitHub sayfasÄ±ndan indirildi. Sistem gereksinimleri karÅŸÄ±landÄ±ktan sonra uygulama baÅŸlatÄ±ldÄ± ve tarayÄ±cÄ± Ã¼zerinden arayÃ¼z aÃ§Ä±ldÄ±.

- Uygulama Ã¼zerinden `settings` kÄ±smÄ±ndan LLM provider olarak Ollama seÃ§ildi. Local olarak kurulu olan Gemma3:4b seÃ§ildi. Bu ayarlarla Anything LLM Ollama'da Ã§alÄ±ÅŸan modeli kullanmaya baÅŸladÄ±.

## â“ SÄ±k Sorulan Sorulara YanÄ±tlar

**LLMâ€™i nereden serve ediyorsun?**

â†’ Ollama Ã¼zerinden http://localhost:11434 adresinden serve ediyorum.

**AnythingLLM kendi iÃ§inde mi model Ã§alÄ±ÅŸtÄ±rÄ±yor?**

â†’ HayÄ±r. Modeli Ã§alÄ±ÅŸtÄ±ran Ollama. AnythingLLM sadece arayÃ¼z ve yÃ¶nlendirme katmanÄ±dÄ±r.

**3rd party LLM servisi kullandÄ±n mÄ±?**

â†’ HayÄ±r. Her ÅŸey local olarak Ã§alÄ±ÅŸÄ±yor. DÄ±ÅŸ baÄŸlantÄ± yok.



